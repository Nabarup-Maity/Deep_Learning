{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nabarup-Maity/Deep_Learning/blob/main/NER_prediction_with_lstm_and_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlzUBjlgazPB"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rQMEE1PDxui"
      },
      "source": [
        "**NER prediction with LSTM and Transformers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4viZuySLD3HD"
      },
      "source": [
        "For this assignment we will be exploring the use of lstms and transformers for named entity recognition (NER) tasks. In this case, we will be looking at recognizing word tagging (e.g., classifying each word as a business, a place, etc...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z8dD6pSEXDm"
      },
      "source": [
        "First, download and upload the ner_dataset.csv file from this site (https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus?select=ner_dataset.csv), we will be using this for experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-Zvwl90EnZJ"
      },
      "source": [
        "Import the libraries we will need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wFWdYWwEqFp"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from itertools import chain\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRyutDB9E0sX"
      },
      "source": [
        "Let's look at the structure of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "yNj70CojE3Ve",
        "outputId": "42710cf5-d0dd-4f8b-ec11-cf88721ec49f"
      },
      "source": [
        "data = pd.read_csv('ner_dataset.csv', encoding= 'unicode_escape')\n",
        "data.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>have</td>\n",
              "      <td>VBP</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>marched</td>\n",
              "      <td>VBN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NaN</td>\n",
              "      <td>through</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NaN</td>\n",
              "      <td>London</td>\n",
              "      <td>NNP</td>\n",
              "      <td>B-geo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>NaN</td>\n",
              "      <td>to</td>\n",
              "      <td>TO</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>NaN</td>\n",
              "      <td>protest</td>\n",
              "      <td>VB</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>NaN</td>\n",
              "      <td>the</td>\n",
              "      <td>DT</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Sentence #           Word  POS    Tag\n",
              "0  Sentence: 1      Thousands  NNS      O\n",
              "1          NaN             of   IN      O\n",
              "2          NaN  demonstrators  NNS      O\n",
              "3          NaN           have  VBP      O\n",
              "4          NaN        marched  VBN      O\n",
              "5          NaN        through   IN      O\n",
              "6          NaN         London  NNP  B-geo\n",
              "7          NaN             to   TO      O\n",
              "8          NaN        protest   VB      O\n",
              "9          NaN            the   DT      O"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jvynmAHFjKx"
      },
      "source": [
        "We next need to create a mapping between tokens, tags, and ids. Each token should map to a unique id, and each tag should map to a unique class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSg1neW0IdnD"
      },
      "source": [
        "Now you might have noticed that each sentece is split into multiple rows. We need to transform this data into sequences of words and tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibNA2AqWZp4Q"
      },
      "source": [
        "# Fill na\n",
        "data_fillna = data.fillna(method='ffill', axis=0)\n",
        "# Groupby and collect columns\n",
        "data_group = data_fillna.groupby(\n",
        "['Sentence #'],as_index=False\n",
        ")['Word', 'POS', 'Tag', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))\n",
        "# Visualise data\n",
        "data_group.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oBPyo8ZJDPm"
      },
      "source": [
        "Next we split the data into training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoT2bYY1ZsNg"
      },
      "source": [
        "#Enter your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPBi9B3vKPfy"
      },
      "source": [
        "Next create the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kg2UnsT0KRfx"
      },
      "source": [
        "# Enter code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joMehFqIK67E"
      },
      "source": [
        "Define the loss function for the task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh3V2M_eK9Un"
      },
      "source": [
        "# Enter code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IKXPtCLLKf9"
      },
      "source": [
        "Train the model. First, find some pre-trained embeddings to help us with the task...for example, you can find GloVe embeddings here https://nlp.stanford.edu/projects/glove/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stbwKuR5LPd9"
      },
      "source": [
        "def load_embeddings():\n",
        "  lines = open(\"glove.6B.100d.txt\", \"r\").readlines()\n",
        "\n",
        "  w2e = {}\n",
        "  for l in lines:\n",
        "    s = l.split(\" \")\n",
        "    word = s[0]\n",
        "    embedding = np.zeros( (1, len(s)-1))\n",
        "    for k, x in enumerate(s[1:]):\n",
        "      embedding[0,k] = float(x.strip())\n",
        "\n",
        "    w2e[word] = embedding\n",
        "\n",
        "  return w2e\n",
        "\n",
        "w2e = load_embeddings()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKui8Vdu74GG"
      },
      "source": [
        "# Enter Code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}